# Copyright (c) 2020, SAS Institute Inc., Cary, NC, USA.  All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import re
from pathlib import Path

import pandas as pd

from .._services.model_repository import ModelRepository as mr
from ..core import current_session

MAS_CODE_NAME = "dmcas_packagescorecode.sas"
CAS_CODE_NAME = "dmcas_epscorecode.sas"


class ScoreCode:
    # TODO: change predict_method to accept Python object instead of string
    score_code = ""

    @classmethod
    def write_score_code(
        cls,
        input_data,
        model_prefix,
        predict_method,
        model_file_name,
        metrics,
        model=None,
        target_values=None,
        predict_threshold=None,
        score_code_path=None,
        score_cas=True,
        pickle_type="pickle",
        **kwargs,
    ):
        """
        Writes a Python score code file based on training data used to generate the model
        pickle file. The Python file is included in the ZIP file that is imported or registered
        into the common model repository. The model can then be used by SAS applications,
        such as SAS Open Model Manager.

        The score code that is generated is designed to be a working template for any
        Python model, but is not guaranteed to work out of the box for scoring, publishing,
        or validating the model.

        Note that for categorical variables, the variable is split into the possible
        categorical values of the variable. Also, by default it does NOT include a catch-all
        [catVar]_Other variable to store any missing values or any values not found in the
        training data set. If you have missing values or values not included in your training
        data set, you must set the OtherVariable option to True.

        Both the input_data and target_data dataframes have the following stipulations:
        * Column names must be a valid Python variable name.
        * For categorical columns, the values must be a valid Python variable name.
        If either of these conditions is broken, an exception is raised.

        The following files are generated by this function:
        * '*Score.py'
            The Python score code file for the model.
        * 'dcmas_epscorecode.sas' (for SAS Viya 3.5 models)
            Python score code wrapped in DS2 and prepared for CAS scoring or publishing.
        * 'dmcas_packagescorecode.sas' (for SAS Viya 3.5 models)
            Python score code wrapped in DS2 and prepared for SAS Microanalyic Service scoring or publishing.

        Parameters
        ----------
        input_data : DataFrame or list of dicts
            The `DataFrame` object contains the training data, and includes only the predictor
            columns. The write_score_code function currently supports int(64), float(64),
            and string data types for scoring. Providing a list of dict objects signals
            that the model files are being created from an MLFlow model.
        target_data : pandas Series
            The `DataFrame Series` object contains the training data for the target variable. Note that
            for MLFlow models, this can be set as None.
        model_prefix : string
            The variable for the model name that is used when naming model files.
            (For example: hmeqClassTree + [Score.py || .pickle]).
        predict_method : string
            User-defined prediction method for score testing. This should be
            in a form such that the model and data input can be added using
            the format() command.
            For example: '{}.predict_proba({})'.
        model_file_name : string
            Name of the model file that contains the model.
        metrics : string list, optional
            The scoring metrics for the model. For classification models, it is assumed that the last value in the list
            represents the classification output. The default is a list of two metrics: EM_EVENTPROBABILITY and
            EM_CLASSIFICATION. The following scenarios are supported:
                1) If only one value is provided, then it is assumed that the model returns either a binary response
                prediction or a character output and is returned as the output.
                2) If only two values are provided, a threshold value needs to be set: either by providing a
                threshold argument or the function taking the mean of the provided target column. Then the
                threshold value sets the classification output for the prediction.
                3) If more than two values are provided, the largest probability is accepted as the event and the
                appropriate classification value is returned for the output.
        score_code_path : string, optional
            The local path of the score code file. The default is the current
            working directory.
        threshold : float, optional
            The prediction threshold for probability metrics. For classification,
            below this threshold is a 0 and above is a 1.
        other_variable : boolean, optional
            The option for having a categorical other value for catching missing
            values or values not found in the training data set. The default setting
            is False.
        model : str or dict
            The name or id of the model, or a dictionary representation of
            the model. The default value is None and is only necessary for models that
            will be hosted on SAS Viya 3.5.
        is_h2o_model : boolean, optional
            Sets whether the model is an H2O.ai Python model. By default False.
        missing_values : boolean, optional
            Sets whether data used for scoring needs to go through imputation for
            missing values before passed to the model. By default False.
        score_cas : boolean, optional
            Sets whether models registered to SAS Viya 3.5 should be able to be scored and
            validated through both CAS and SAS Micro Analytic Service. By default true. If
            set to false, then the model will only be able to be scored and validated through
            SAS Micro Analytic Service. By default True.
        is_binary_model : boolean, optional
            Sets whether the H2O model provided is a binary model or a MOJO model. By default False.
        binary_string : string, optional
            Binary string representation of the model object. By default None.
        pickle_type : string, optional
            Indicator for MLFlow models, which may pickle by non-standard methods. By default 'pickle'.
        """
        # Set default metrics if no value is given
        if not metrics:
            metrics = ["EM_EVENTPROBABILITY", "EM_CLASSIFICATION"]

        if isinstance(input_data, pd.DataFrame):
            # From the input dataframe columns, create a list of input variables,
            # then check for viability
            input_var_list = input_data.columns.to_list()
            cls._check_for_invalid_variable_names(input_var_list)
            input_dtypes_list = input_data.dtypes.astype(str).to_list()
        else:
            # For MLFlow models, extract the variables and data types
            input_var_list = [var["name"] for var in input_data]
            cls._check_for_invalid_variable_names(input_var_list)
            input_dtypes_list = [var["type"] for var in input_data]

        # For SAS Viya 3.5, either return an error or return the model UUID as a string
        if current_session().version_info() == 3.5:
            model_id = cls._get_model_id(model)
        else:
            model_id = None

        # Set the model_file_name based on kwargs input
        if "model_file_name" in kwargs:
            model_file_name = kwargs["model_file_name"]
            binary_string = None
        elif "binary_string" in kwargs:
            model_file_name = None
            binary_string = kwargs["binary_string"]
        else:
            binary_string = None

        # Add the core imports to the score code with the specified model serializer
        cls._write_imports(
            pickle_type,
            mojo_model="mojo_model" in kwargs,
            binary_h2o_model="binary_h2o_model" in kwargs,
            binary_string=binary_string,
        )

        # Generate model loading code for SAS Viya 3.5 models without binary strings
        if model_id and not binary_string:
            model_load = cls._viya35_model_load(
                model_id,
                pickle_type,
                model_file_name,
                mojo_model="mojo_model" in kwargs,
                binary_h2o_model="binary_h2o_model" in kwargs,
            )
        # As above, but for SAS Viya 4 models
        elif not binary_string:
            model_load = cls._viya4_model_load(
                pickle_type,
                model_file_name,
                mojo_model="mojo_model" in kwargs,
                binary_h2o_model="binary_h2o_model" in kwargs,
            )
        else:
            model_load = None

        # Define the score function using the variables found in input_data
        # Set the output variables in the line below from metrics
        cls.score_code += (
            f"def score{model_prefix}({', '.join(input_var_list)}):\n"
            f"{'':4}Output: {', '.join(metrics)}"
        )

        # Run a try/except block to catch errors for model loading (skip binary string)
        if model_load:
            cls.score_code += (
                f"{'':4}try:\n{'':8}global model\n{'':4}"
                f"except NameError:\n{model_load}"
            )

        if "missing_values" in kwargs:
            cls._impute_missing_values(input_data, input_var_list, input_dtypes_list)

        # Create the appropriate style of input array and write out the predict method
        if any(x in ["mojo_model", "binary_h2o_model"] for x in kwargs):
            cls._predict_method(
                predict_method, input_var_list, dtypes_list=input_dtypes_list
            )
            cls._predictions_to_metrics(
                metrics,
                target_values=target_values,
                predict_threshold=predict_threshold,
                h2o_model=True,
            )
        else:
            cls._predict_method(
                predict_method,
                input_var_list,
                statsmodels_model="statsmodels_model" in kwargs,
            )

        if model_id:
            files = [
                {
                    "name": f"{model_prefix}_score.py",
                    "file": cls.score_code,
                    "role": "score",
                }
            ]
            cls.upload_and_copy_score_resources(model_id, files)
            mr.convert_python_to_ds2(model_id)
            if score_cas:
                model_contents = mr.get_model_contents(model_id)
                for file in model_contents:
                    if file.name == "score.sas":
                        mas_code = mr.get(f"models/{file.modelId}/contents/{file.id}")
                        cls.upload_and_copy_score_resources(
                            model_id,
                            [
                                {
                                    "name": MAS_CODE_NAME,
                                    "file": mas_code,
                                    "role": "score",
                                }
                            ],
                        )
                        cas_code = cls.convert_mas_to_cas(mas_code, model_id)
                        cls.upload_and_copy_score_resources(
                            model_id,
                            [
                                {
                                    "name": CAS_CODE_NAME,
                                    "file": cas_code,
                                    "role": "score",
                                }
                            ],
                        )
                        model = mr.get_model(model_id)
                        model["scoreCodeType"] = "ds2MultiType"
                        mr.update_model(model)
                        break

        if score_code_path:
            py_code_path = Path(score_code_path) / (model_prefix + "_score.py")
            with open(py_code_path, "w") as py_file:
                py_file.write(cls.score_code)
            if model_id and score_cas:
                with open(Path(score_code_path) / MAS_CODE_NAME, "w") as sas_file:
                    # noinspection PyUnboundLocalVariable
                    sas_file.write(mas_code)
                with open(Path(score_code_path) / CAS_CODE_NAME, "w") as sas_file:
                    # noinspection PyUnboundLocalVariable
                    sas_file.write(cas_code)
        else:
            output_dict = {model_prefix + "_score.py": cls.score_code}
            if model_id and score_cas:
                # noinspection PyUnboundLocalVariable
                output_dict[MAS_CODE_NAME] = mas_code
                # noinspection PyUnboundLocalVariable
                output_dict[CAS_CODE_NAME] = cas_code
            return output_dict

    @staticmethod
    def upload_and_copy_score_resources(model, files):
        for file in files:
            mr.add_model_content(model, **file)
        return mr.copy_python_resources(model)

    @staticmethod
    def _get_model_id(model):
        """"""
        if not model:
            raise ValueError(
                "No model identification was provided. Python score code"
                " generation for SAS Viya 3.5 requires the model's UUID."
            )
        else:
            model_response = mr.get_model(model)
            try:
                model_id = model_response["id"]
            except TypeError:
                raise ValueError(
                    "No model could be found using the model argument provided."
                )
        return model_id

    @staticmethod
    def _check_for_invalid_variable_names(var_list):
        """"""
        for name in var_list:
            if not str(name).isidentifier():
                raise SyntaxError(
                    f"{str(name)} is not a valid variable name. Please confirm that all"
                    " variable names can be used as Python variables."
                    " E.g. `str(name).isidentifier() == True`."
                )

    @classmethod
    def _write_imports(
        cls, pickle_type, mojo_model=False, binary_h2o_model=False, binary_string=None
    ):
        """"""
        pickle_type = pickle_type if pickle_type else "pickle"
        cls.score_code += (
            f"import math\nimport {pickle_type}\nimport pandas as pd\n"
            "import numpy as np\nfrom pathlib import Path\n\n"
        )
        if current_session().version_info() != 3.5:
            cls.score_code += "import settings\n\n"
        if mojo_model or binary_h2o_model:
            cls.score_code += (
                "import h2o\nimport gzip\nimport shutil\nimport os\n\n" "h2o.init()\n\n"
            )
        elif binary_string:
            cls.score_code += (
                f'import codecs\n\nbinary_string = "{binary_string}"'
                f"\nmodel = pickle.loads(codecs.decode(binary_string"
                '.encode(), "base64"))\n\n'
            )

    @classmethod
    def _viya35_model_load(
        cls,
        model_id,
        pickle_type,
        model_file_name=None,
        mojo_model=False,
        binary_h2o_model=False,
    ):
        """"""
        if mojo_model:
            cls.score_code += (
                f'model_path = Path("/models/resources/viya/{model_id}'
                f'")\nwith gzip.open(model_path / "{model_file_name}'
                f'", "r") as fileIn, open(model_path / '
                f"\"{str(Path(model_file_name).with_suffix('.zip'))}\","
                " \"wb\") as fileOut:\n{'':4}shutil.copyfileobj(fileIn,"
                " fileOut)\nos.chmod(model_path / "
                f"\"{str(Path(model_file_name).with_suffix('.zip'))}\""
                ", 0o777)\nmodel = h2o.import_mojo(model_path / "
                f"\"{str(Path(model_file_name).with_suffix('.zip'))}\")"
                "\n\n"
            )
            return (
                f"{'':8}model = h2o.import_mojo(model_path / \""
                f"{str(Path(model_file_name).with_suffix('.zip'))}\")"
            )
        elif binary_h2o_model:
            cls.score_code += (
                'model = h2o.load(Path("/models/resources/viya/'
                f'{model_id}/{model_file_name}"))\n\n'
            )
            return (
                f'        model = h2o.load(Path("/models/resources/viya/'
                f'{model_id}/{model_file_name}"))'
            )
        else:
            cls.score_code += (
                f'model_path = Path("/models/resources/viya/{model_id}'
                f'")\nwith open(model_path / "{model_file_name}", '
                f"\"rb\") as pickle_model:\n{'':4}model = {pickle_type}"
                ".load(pickle_model)\n\n"
            )
            return (
                f"{'':8}model_path = Path(\"/models/resources/viya/{model_id}"
                f"\")\n{'':8}with open(model_path / \"{model_file_name}\", "
                f"\"rb\") as pickle_model:\n{'':12}model = {pickle_type}"
                ".load(pickle_model)"
            )

    @classmethod
    def _viya4_model_load(
        cls, pickle_type, model_file_name=None, mojo_model=False, binary_h2o_model=False
    ):
        """"""
        if mojo_model:
            cls.score_code += (
                f"with gzip.open(Path(settings.pickle_path) / "
                '"{model_file_name}", "r") as fileIn, '
                "open(Path(settings.pickle_path) / "
                f"\"{str(Path(model_file_name).with_suffix('.zip'))}\","
                " \"wb\") as fileOut:\n{'':4}shutil.copyfileobj(fileIn,"
                " fileOut)\nos.chmod(Path(settings.pickle_path) / "
                f"\"{str(Path(model_file_name).with_suffix('.zip'))}\""
                ", 0o777)\nmodel = h2o.import_mojo("
                "Path(settings.pickle_path) / "
                f"\"{str(Path(model_file_name).with_suffix('.zip'))}\")"
                "\n\n"
            )
            return (
                f"{'':8}model = h2o.import_mojo(Path(settings.pickle_path) / "
                f"\"{str(Path(model_file_name).with_suffix('.zip'))}\")"
            )
        elif binary_h2o_model:
            cls.score_code += "model = h2o.load(Path(settings.pickle_path))\n\n"
            return f"{'':8}model = h2o.load(Path(settings.pickle_path))"
        else:
            cls.score_code += (
                f"with open(Path(settings.pickle_path) / "
                f'"{model_file_name}", "rb") as pickle_model:\n    '
                f"model = {pickle_type}.load(pickle_model)\n\n"
            )
            return (
                f"{'':8}with open(Path(settings.pickle_path) / "
                f'"{model_file_name}", "rb") as pickle_model:\n    '
                f"{'':12}model = {pickle_type}.load(pickle_model)"
            )

    @classmethod
    def _impute_missing_values(cls, data, var_list, dtype_list):
        """"""
        for i, (var, dtype) in enumerate(zip(var_list, dtype_list)):
            # Split up between numeric and character variables
            if any(t in dtype for t in ["int", "float"]):
                cls._impute_numeric(data, var)
            else:
                cls._impute_char(var)

    @classmethod
    def _impute_numeric(cls, data, var):
        """"""
        # If binary values, then compute the mode instead of the mean
        if data[var].isin([0, 1]).all():
            cls.score_code += (
                f"{'':4}try:\n{'':8}if math.isnan({var}):\n"
                f"{'':12}{var} = {data[var].mode()[0]}\n"
                f"{'':4}except TypeError:\n{'':8}{var} = "
                f"{data[var].mode()[0]}\n"
            )
        else:
            cls.score_code += (
                f"{'':4}try:\n{'':8}if math.isnan({var}):\n"
                f"{'':12}{var} = {data[var].mean()}\n"
                f"{'':4}except TypeError\n{'':8}{var} = "
                f"{data[var].mean()}\n"
            )

    @classmethod
    def _impute_char(cls, var):
        """"""
        # Replace non-string values with blank strings
        cls.score_code += (
            f"{'':4}try:\n{'':8}{var} = {var}.strip()\n{'':4}except "
            f"AttributeError:\n{'':8}{var} = \"\""
        )

    @classmethod
    def _predict_method(
        cls, method, var_list, dtypes_list=None, statsmodels_model=None
    ):
        """"""
        column_names = ", ".join("'%s'" % col for col in var_list)
        # H2O models
        if dtypes_list:
            column_types = []
            for (var, dtype) in zip(var_list, dtypes_list):
                if any(x in dtype for x in ["int", "float"]):
                    col_type = "numeric"
                else:
                    col_type = "string"
                column_types.append(f'"{var}" : "{col_type}"')
            cls.score_code += (
                f"{'':4}input_array = pd.DataFrame("
                f"[[{', '.join(var_list)}]],\n{'':31}columns=["
                f"{column_names}],\n{'':31}dtype=float,\n{'':31}"
                f"index=[0])\n{'':4}column_types = {{{column_types}}}\n"
                f"{'':4}h2o_array = h2o.H2OFrame(input_array, "
                f"column_types=column_types)\n{'':4}prediction = "
                f"model.{method.__name__}(h2o_array)\n{'':4}prediction"
                f" = h2o.as_list(prediction, use_pandas=False)\n"
            )
        # Statsmodels models
        elif statsmodels_model:
            cls.score_code += (
                f"{'':4}inputArray = pd.DataFrame("
                f"[[1.0, {', '.join(var_list)}]],\n{'':29}columns=["
                f"\"const\", {column_names}],\n{'':29}dtype=float)\n"
                f"{'':4}prediction = model.{method.__name__}"
                f"(input_array)\n"
            )
        else:
            cls.score_code += (
                f"{'':4}input_array = pd.DataFrame("
                f"[[{', '.join(var_list)}]],\n{'':30}columns=["
                f"{column_names}],\n{'':30}dtype=float)\n{'':4}"
                f"prediction = model.{method.__name__}(input_array)\n"
            )

    @classmethod
    def _predictions_to_metrics(
        cls, metrics, target_values=None, predict_threshold=None, h2o_model=None
    ):
        """

        Parameters
        ----------
        metrics : string list
            A list of strings corresponding to the outputs of the model to SAS Model
            Manager.
        target_values : list of int, float, string
            A list of target values for the target variable. Default is None.
        predict_threshold : list of floats
            A list of target value thresholds. Default is None.
        """
        if len(metrics) == 1 and isinstance(metrics, list):
            # Flatten single valued list
            metrics = metrics[0]

        if not (target_values or predict_threshold):
            cls._no_targets_no_thresholds(metrics, h2o_model)
        elif int(target_values) == 1:
            cls._binary_target(metrics, predict_threshold, h2o_model)
        elif len(target_values) > 1:
            cls._nonbinary_targets(metrics, target_values, h2o_model)
        elif len(target_values) == 1 and int(target_values) != 1:
            raise ValueError(
                "For non-binary target variables, please provide at"
                " least two target values."
            )
        elif not target_values and predict_threshold:
            raise ValueError(
                "A threshold was provided to interpret the prediction "
                "results, however a target value was not, therefore, "
                "a valid output cannot be generated."
            )

    @classmethod
    def _no_targets_no_thresholds(cls, metrics, h2o_model=None):
        """"""
        if len(metrics) == 1:
            # Assume no probability output & predict function returns classification
            if h2o_model:
                cls.score_code += (
                    f"{'':4}{metrics} = prediction[1][0]\n\n{'':4}" f"return {metrics}"
                )
            else:
                cls.score_code += (
                    f"{'':4}{metrics} = prediction\n\n{'':4}" f"return {metrics}"
                )
        else:
            if h2o_model:
                cls.score_code += f"{'':4}{metrics[0]} = prediction[1][0]\n"
                for i in range(len(metrics) - 1):
                    cls.score_code += (
                        f"{'':4}{metrics[i + 1]} = prediction[1]" f"[{i + 1}]\n"
                    )
            else:
                # Assume predict call returns (classification, probabilities)
                cls.score_code += f"{'':4}{metrics[0]} = prediction[0]\n"
                for i in range(len(metrics) - 1):
                    cls.score_code += f"{'':4}{metrics[i + 1]} = prediction[{i + 1}]\n"
            cls.score_code += f"\n{'':4}return {', '.join(metrics)}"

    @classmethod
    def _binary_target(cls, metrics, threshold=None, h2o_model=None):
        """"""
        # If a binary target value is provided, then classify the prediction
        if not threshold:
            # Set default threshold
            threshold = 0.5
        if len(metrics) == 1:
            if h2o_model:
                cls.score_code += (
                    f"{'':4}if prediction[1][2] > {threshold}:\n"
                    f"{'':8}{metrics} = 1\n{'':4}else:\n{'':8}"
                    f"{metrics} = 0\n\nreturn {metrics}"
                )
            else:
                cls.score_code += (
                    f"{'':4}if prediction > {threshold}:\n"
                    f"{'':8}{metrics} = 1\n{'':4}else:\n{'':8}"
                    f"{metrics} = 0\n\nreturn {metrics}"
                )
        elif len(metrics) == 2:
            if h2o_model:
                cls.score_code += (
                    f"{'':4}if prediction[1][2] > {threshold}:\n"
                    f"{'':8}{metrics[0]} = 1\n{'':4}else:\n{'':8}"
                    f"{metrics[0]} = 0\n\nreturn {metrics[0]}, "
                    f"prediction[1][2]"
                )
            else:
                cls.score_code += (
                    f"{'':4}if prediction > {threshold}:\n"
                    f"{'':8}{metrics[0]} = 1\n{'':4}else:\n{'':8}"
                    f"{metrics[0]} = 0\n\nreturn {metrics[0]}, prediction"
                )
        else:
            raise ValueError("Too many metrics were provided for a binary model.")

    @classmethod
    def _nonbinary_targets(cls, metrics, target_values, h2o_model=None):
        """"""
        # Find the target value with the highest probability
        if len(metrics) == 1:
            if h2o_model:
                cls.score_code += (
                    f"{'':4}target_values = {target_values}\n{'':4}"
                    f"{metrics} = target_values[prediction[1][1:]."
                    f"index(max(prediction[1][1:]))]\n{'':4}"
                    f"return {metrics}"
                )
            else:
                cls.score_code += (
                    f"{'':4}target_values = {target_values}\n{'':4}"
                    f"{metrics} = target_values[prediction.index("
                    f"max(prediction))]\n{'':4}return {metrics}"
                )
        else:
            if h2o_model:
                cls.score_code += (
                    f"{'':4}target_values = {target_values}\n{'':4}"
                    f"{metrics} = target_values[prediction[1][1:]."
                    f"index(max(prediction[1][1:]))]\n{'':4}"
                )
                for i in range(len(metrics) - 1):
                    cls.score_code += (
                        f"{'':4}{metrics[i + 1]} = " f"prediction[1][{i + 1}]\n"
                    )
            else:
                cls.score_code += (
                    f"{'':4}target_values = {target_values}\n{'':4}"
                    f"{metrics[0]} = target_values[prediction.index("
                    f"max(prediction))]\n"
                )
                for i in range(len(metrics) - 1):
                    cls.score_code += f"{'':4}{metrics[i + 1]} = prediction[{i + 1}]\n"
            cls.score_code += f"{'':4}return {', '.join(metrics)}"

    @staticmethod
    def convert_mas_to_cas(mas_code, model):
        """
        Using the generated score.sas code from the Python wrapper API, convert the
        SAS Microanalytic Service based code to CAS compatible.

        Parameters
        ----------
        mas_code : str
            String representation of the packagescore.sas DS2 wrapper
        model : str or dict
            The name or id of the model, or a dictionary representation of
            the model

        Returns
        -------
        CASCode : str
            String representation of the epscorecode.sas DS2 wrapper code
        """
        model = mr.get_model(model)
        output_string = ""
        for out_var in model["outputVariables"]:
            output_string = output_string + "dcl "
            if out_var["type"] == "string":
                output_string = output_string + "varchar(100) "
            else:
                output_string = output_string + "double "
            output_string = output_string + out_var["name"] + ";\n"
        start = mas_code.find("score(")
        finish = mas_code[start:].find(");")
        score_vars = mas_code[start + 6 : start + finish]
        input_string = " ".join(
            [
                x
                for x in score_vars.split(" ")
                if (x != "double" and x != "in_out" and x != "varchar(100)")
            ]
        )
        end_block = (
            f"method run();\n{'':4}set SASEP.IN;\n{'':4}score({input_string});\nend;"
            f"\nenddata;"
        )
        replace_strings = {
            "package pythonScore / overwrite=yes;": "data sasep.out;",
            "dcl int resultCode revision;": "dcl double resultCode revision;\n"
            + output_string,
            "endpackage;": end_block,
        }
        replace_strings = dict((re.escape(k), v) for k, v in replace_strings.items())
        pattern = re.compile("|".join(replace_strings.keys()))
        cas_code = pattern.sub(
            lambda m: replace_strings[re.escape(m.group(0))], mas_code
        )
        return cas_code
