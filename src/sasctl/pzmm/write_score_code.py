# Copyright (c) 2020, SAS Institute Inc., Cary, NC, USA.  All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import re
from pathlib import Path

from .._services.model_repository import ModelRepository as modelRepo
from ..core import current_session


class ScoreCode:
    # TODO: change predict_method to accept Python object instead of string
    @classmethod
    def write_score_code(
        cls,
        input_data,
        target_df,
        model_prefix,
        predict_method,
        model_file_name,
        metrics=None,
        score_code_path=Path.cwd(),
        threshold=None,
        other_variable=False,
        model=None,
        is_h2o_model=False,
        missing_values=False,
        score_cas=True,
        is_binary_model=False,
        binary_string=None,
        pickle_type="pickle",
    ):
        """
        Writes a Python score code file based on training data used to generate the model
        pickle file. The Python file is included in the ZIP file that is imported or registered
        into the common model repository. The model can then be used by SAS applications,
        such as SAS Open Model Manager.

        The score code that is generated is designed to be a working template for any
        Python model, but is not guaranteed to work out of the box for scoring, publishing,
        or validating the model.

        Note that for categorical variables, the variable is split into the possible
        categorical values of the variable. Also, by default it does NOT include a catch-all
        [catVar]_Other variable to store any missing values or any values not found in the
        training data set. If you have missing values or values not included in your training
        data set, you must set the OtherVariable option to True.

        Both the inputData and target_df dataframes have the following stipulations:
        * Column names must be a valid Python variable name.
        * For categorical columns, the values must be a valid Python variable name.
        If either of these conditions is broken, an exception is raised.

        The following files are generated by this function:
        * '*Score.py'
            The Python score code file for the model.
        * 'dcmas_epscorecode.sas' (for SAS Viya 3.5 models)
            Python score code wrapped in DS2 and prepared for CAS scoring or publishing.
        * 'dmcas_packagescorecode.sas' (for SAS Viya 3.5 models)
            Python score code wrapped in DS2 and prepared for SAS Microanalyic Service scoring or publishing.

        Parameters
        ----------
        input_data : DataFrame or list of dicts
            The `DataFrame` object contains the training data, and includes only the predictor
            columns. The write_score_code function currently supports int(64), float(64),
            and string data types for scoring. Providing a list of dict objects signals
            that the model files are being created from an MLFlow model.
        target_df : pandas Series
            The `DataFrame Series` object contains the training data for the target variable. Note that
            for MLFlow models, this can be set as None.
        model_prefix : string
            The variable for the model name that is used when naming model files.
            (For example: hmeqClassTree + [Score.py || .pickle]).
        predict_method : string
            User-defined prediction method for score testing. This should be
            in a form such that the model and data input can be added using
            the format() command.
            For example: '{}.predict_proba({})'.
        model_file_name : string
            Name of the model file that contains the model.
        metrics : string list, optional
            The scoring metrics for the model. For classification models, it is assumed that the last value in the list
            represents the classification output. The default is a list of two metrics: EM_EVENTPROBABILITY and
            EM_CLASSIFICATION. The following scenarios are supported:
                1) If only one value is provided, then it is assumed that the model returns either a binary response
                prediction or a character output and is returned as the output.
                1) If only two values are provided, a threshold value needs to be set: either by providing a
                threshold argument or the function taking the mean of the provided target column. Then the
                threshold value sets the classification output for the prediction.
                2) If more than two values are provided, the largest probability is accepted as the event and the
                appropriate classification value is returned for the output.
        score_code_path : string, optional
            The local path of the score code file. The default is the current
            working directory.
        threshold : float, optional
            The prediction threshold for probability metrics. For classification,
            below this threshold is a 0 and above is a 1.
        other_variable : boolean, optional
            The option for having a categorical other value for catching missing
            values or values not found in the training data set. The default setting
            is False.
        model : str or dict
            The name or id of the model, or a dictionary representation of
            the model. The default value is None and is only necessary for models that
            will be hosted on SAS Viya 3.5.
        is_h2o_model : boolean, optional
            Sets whether the model is an H2O.ai Python model. By default False.
        missing_values : boolean, optional
            Sets whether data used for scoring needs to go through imputation for
            missing values before passed to the model. By default False.
        score_cas : boolean, optional
            Sets whether models registered to SAS Viya 3.5 should be able to be scored and
            validated through both CAS and SAS Micro Analytic Service. By default true. If
            set to false, then the model will only be able to be scored and validated through
            SAS Micro Analytic Service. By default True.
        is_binary_model : boolean, optional
            Sets whether the H2O model provided is a binary model or a MOJO model. By default False.
        binary_string : string, optional
            Binary string representation of the model object. By default None.
        pickle_type : string, optional
            Indicator for MLFlow models, which may pickle by non-standard methods. By default 'pickle'.
        """
        # Set metrics internal to function call if no value is given
        if not metrics:
            metrics = ["EM_EVENTPROBABILITY", "EM_CLASSIFICATION"]

        # Check if binary string model
        if binary_string:
            is_binary_string = True
        else:
            is_binary_string = False

        # Check if MLFlow Model
        if isinstance(input_data, list):
            is_mlflow = True
        else:
            is_mlflow = False

        # Call REST API to check SAS Viya version
        is_viya35 = current_session().version_info() == 3.5

        # Initialize model_id to remove unbound variable warnings
        model_id = None

        # Helper method for uploading & migrating files
        # TODO: Integrate with register_model() or publish_model() task.
        def upload_and_copy_score_resources(model, files):
            for file in files:
                modelRepo.add_model_content(model, **file)
            return modelRepo.copy_python_resources(model)

        # For SAS Viya 3.5, either return an error or return the model UUID as a string
        if is_viya35:
            if model is None:
                raise ValueError(
                    "The model UUID is required for score code written for"
                    + " SAS Model Manager on SAS Viya 3.5."
                )
            elif modelRepo.is_uuid(model):
                model_id = model
            elif isinstance(model, dict) and "id" in model:
                model_id = model["id"]
            else:
                model = modelRepo.get_model(model)
                model_id = model["id"]

        if not is_mlflow:
            # From the input dataframe columns, create a list of input variables, 
            # then check for viability
            input_var_list = list(input_data.columns)
            for name in input_var_list:
                if not str(name).isidentifier():
                    raise SyntaxError(
                        "Invalid column name in input_data. Columns must be "
                        + "valid as Python variables."
                    )
            new_var_list = list(input_var_list)
            input_dtypes_list = list(input_data.dtypes)
        elif is_mlflow:
            input_var_list = [var["name"] for var in input_data]
            for name in input_var_list:
                if not str(name).isidentifier():
                    raise SyntaxError(
                        "Invalid column name in input_data. Columns must be "
                        + "valid as Python variables."
                    )
            new_var_list = input_var_list
            input_dtypes_list = [var["type"] for var in input_data]

        # Set the location for the Python score file to be written, then open the file
        model_dir = Path(score_code_path)
        score_code_path = Path(score_code_path) / (model_prefix + "Score.py")
        with open(score_code_path, "w") as cls.score_file:

            # For H2O models, include the necessary packages
            if is_h2o_model:
                cls.score_file.write(
                    """\
import h2o
import gzip, shutil, os"""
                )
            # For binary string models, include the necessary packages
            if is_binary_string:
                cls.score_file.write(
                    """\
import codecs"""
                )
            # Import math for imputation; pickle for serialized models; pandas for 
            # data management; numpy for computation
            cls.score_file.write(
                """\
import math
import {pickleType}
import pandas as pd
import numpy as np""".format(
                    pickleType=pickle_type
                )
            )
            # In SAS Viya 4, a settings.py file is generated that points to the resource 
            # location
            if not is_viya35:
                cls.score_file.write(
                    """\n
import settings
from pathlib import Path"""
                )

            # For H2O models, include the server initialization, or h2o.connect() call to use an H2O server
            if is_h2o_model:
                cls.score_file.write(
                    """\n
h2o.init()"""
                )

            # For each case of SAS Viya version and H2O model or not, load the model file as variable _thisModelFit
            if is_binary_string:
                cls.score_file.write(
                    '''\n
binary_string = """{binaryString}"""
_thisModelFit = pickle.loads(codecs.decode(binary_string.encode(), 'base64'))'''.format(
                        binaryString=binary_string
                    )
                )
            elif is_viya35 and is_h2o_model and not is_binary_model:
                try:
                    cls.score_file.write(
                        """\n
with gzip.open('/models/resources/viya/{modelID}/{modelFileName}', 'r') as fileIn, open('/models/resources/viya/{modelID}/{modelZipFileName}', 'wb') as fileOut:
    shutil.copyfileobj(fileIn, fileOut)
os.chmod('/models/resources/viya/{modelID}/{modelZipFileName}', 0o777)
_thisModelFit = h2o.import_mojo('/models/resources/viya/{modelID}/{modelZipFileName}')""".format(
                            modelID=model_id,
                            modelFileName=model_file_name,
                            modelZipFileName=model_file_name[:-4] + "zip",
                        )
                    )
                except AttributeError:
                    raise ValueError(
                        "The following is not a valid model to register in this format. "
                        + "Please verify that the appropriate arguments have been provided "
                        + "to the import model function."
                    )
            elif is_viya35 and not is_h2o_model:
                cls.score_file.write(
                    """\n
with open('/models/resources/viya/{modelID}/{modelFileName}', 'rb') as _pFile:
    _thisModelFit = {pickleType}.load(_pFile)""".format(
                        modelID=model_id,
                        modelFileName=model_file_name,
                        pickleType=pickle_type,
                    )
                )
            elif is_viya35 and is_binary_model:
                cls.score_file.write(
                    """\n
_thisModelFit = h2o.load_model('/models/resources/viya/{modelID}/{modelFileName}')""".format(
                        modelID=model_id, modelFileName=model_file_name
                    )
                )
            elif not is_viya35 and not is_h2o_model:
                cls.score_file.write(
                    """\n
with open(Path(settings.pickle_path) / '{modelFileName}', 'rb') as _pFile:
    _thisModelFit = {pickleType}.load(_pFile)""".format(
                        modelFileName=model_file_name, pickleType=pickle_type
                    )
                )
            elif not is_viya35 and is_binary_model:
                cls.score_file.write(
                    """\n
_thisModelFit = h2o.load_model(Path(settings.pickle_path) / '{}')""".format(
                        modelFileName=model_file_name
                    )
                )
            elif not is_viya35 and is_h2o_model and not is_binary_model:
                cls.score_file.write(
                    """\n
with gzip.open(Path(settings.pickle_path) / '{modelFileName}', 'r') as fileIn, open(Path(settings.pickle_path) / '{
modelZipFileName}', 'wb') as fileOut:
    shutil.copyfileobj(fileIn, fileOut)
os.chmod(Path(settings.pickle_path) / '{modelZipFileName}', 0o777)
_thisModelFit = h2o.import_mojo(Path(settings.pickle_path) / '{modelZipFileName}')""".format(
                        modelFileName=model_file_name,
                        modelZipFileName=model_file_name[:-4] + "zip",
                    )
                )
            # Create the score function with variables from the input dataframe provided and create the output variable line for SAS Model Manager
            cls.score_file.write(
                '''\n
def score{modelPrefix}({inputVarList}):
    "Output: {metrics}"'''.format(
                    modelPrefix=model_prefix,
                    inputVarList=", ".join(input_var_list),
                    metrics=", ".join(metrics),
                )
            )
            # As a check for missing model variables, run a try/except block that reattempts to load the model in as a variable
            if not is_binary_string:
                cls.score_file.write(
                    """\n
    try:
        global _thisModelFit
    except NameError:\n"""
                )
                if is_viya35 and not is_h2o_model:
                    cls.score_file.write(
                        """
        with open('/models/resources/viya/{modelID}/{modelFileName}', 'rb') as _pFile:
            _thisModelFit = {pickleType}.load(_pFile)""".format(
                            modelID=model_id,
                            modelFileName=model_file_name,
                            pickleType=pickle_type,
                        )
                    )
                elif is_viya35 and is_h2o_model and not is_binary_model:
                    cls.score_file.write(
                        """
        _thisModelFit = h2o.import_mojo('/models/resources/viya/{modelID}/{modelZipFileName}')
        """.format(
                            modelID=model_id, modelZipFileName=model_file_name[:-4] + "zip"
                        )
                    )
                elif is_viya35 and is_binary_model:
                    cls.score_file.write(
                        """
        _thisModelFit = h2o.load_model('/models/resources/viya/{modelID}/{modelFileName}')""".format(
                            modelID=model_id, modelFileName=model_file_name
                        )
                    )
                elif not is_viya35 and not is_h2o_model:
                    cls.score_file.write(
                        """
        with open(Path(settings.pickle_path) / '{modelFileName}', 'rb') as _pFile:
            _thisModelFit = {pickleType}.load(_pFile)""".format(
                            modelFileName=model_file_name, pickleType=pickle_type
                        )
                    )
                elif not is_viya35 and is_h2o_model:
                    cls.score_file.write(
                        """
        _thisModelFit = h2o.import_mojo(Path(settings.pickle_path) / '{}')""".format(
                            model_file_name[:-4] + "zip"
                        )
                    )
                elif not is_viya35 and is_binary_model:
                    cls.score_file.write(
                        """\n
        _thisModelFit = h2o.load_model(Path(settings.pickle_path) / '{}')""".format(
                            modelFileName=model_file_name
                        )
                    )

            if (
                missing_values and not is_mlflow
            ):  # MLFlow models are not guaranteed to have example input data
                # For each input variable, impute for missing values based on variable dtype
                for i, dtypes in enumerate(input_dtypes_list):
                    dtypes = dtypes.name
                    if "int" in dtypes or "float" in dtypes:
                        if cls.check_if_binary(input_data[input_var_list[i]]):
                            cls.score_file.write(
                                """\n
    try:
        if math.isnan({inputVar}):
            {inputVar} = {inputVarMode}
    except TypeError:
        {inputVar} = {inputVarMode}""".format(
                                    inputVar=input_var_list[i],
                                    inputVarMode=float(
                                        list(input_data[input_var_list[i]].mode())[0]
                                    ),
                                )
                            )
                        else:
                            cls.score_file.write(
                                """\n
    try:
        if math.isnan({inputVar}):
            {inputVar} = {inputVarMean}
    except TypeError:
        {inputVar} = {inputVarMean}""".format(
                                    inputVar=input_var_list[i],
                                    inputVarMean=float(
                                        input_data[input_var_list[i]].mean(
                                            axis=0, skipna=True
                                        )
                                    ),
                                )
                            )
                    elif "str" in dtypes or "object" in dtypes:
                        cls.score_file.write(
                            """\n
    try:
        categoryStr = {inputVar}.strip()
    except AttributeError:
        categoryStr = 'Other'\n""".format(
                                inputVar=input_var_list[i]
                            )
                        )

                        temp_var = cls.split_string_column(
                            input_data[input_var_list[i]], other_variable
                        )
                        new_var_list.remove(input_var_list[i])
                        new_var_list.extend(temp_var)
            # For non-H2O models, insert the model into the provided predict_method call
            if not is_h2o_model:
                try:
                    predict_method = predict_method.format("_thisModelFit", "inputArray")
                except AttributeError:
                    raise ValueError(
                        "The following is not a valid model to register in this format. "
                        + "Please verify that the appropriate arguments have been provided "
                        + "to the import model function."
                    )
                cls.score_file.write(
                    """\n
    try:
        inputArray = pd.DataFrame([[{newVars}]],
                                  columns=[{columns}],
                                  dtype=float)
        prediction = {predictMethod}
    except ValueError:
    # For models requiring or including an intercept value, a 'const' column is required
    # For example, many statsmodels models include an intercept value that must be included for the model prediction
        inputArray = pd.DataFrame([[1.0, {newVars}]],
                                columns=['const', {columns}],
                                dtype=float)
        prediction = {predictMethod}""".format(
                        newVars=", ".join(new_var_list),
                        columns=", ".join("'%s'" % x for x in new_var_list),
                        predictMethod=predict_method,
                    )
                ),
            elif is_h2o_model:
                column_type = []
                for (var, dtype) in zip(new_var_list, input_dtypes_list):
                    if "string" in dtype.name:
                        type = "string"
                    else:
                        type = "numeric"
                    column_type.append("'" + var + "'" + ":" + "'" + type + "'")
                cls.score_file.write(
                    """\n
    inputArray = pd.DataFrame([[{newVars}]],
                              columns=[{columns}],
                              dtype=float, index=[0])
    columnTypes = {{{columnTypes}}}
    h2oArray = h2o.H2OFrame(inputArray, column_types=columnTypes)
    prediction = _thisModelFit.predict(h2oArray)
    prediction = h2o.as_list(prediction, use_pandas=False)""".format(
                        newVars=", ".join(new_var_list),
                        columns=", ".join("'%s'" % x for x in new_var_list),
                        columnTypes=", ".join(column_type),
                    )
                )
            if not is_h2o_model and not is_mlflow:
                # TODO: Refactor arguments to better handle different classification types
                if len(metrics) == 1:
                    # For models that output the classification from the prediction
                    cls.score_file.write(
                        """\n
    {metric} = prediction""".format(
                            metric=metrics[0]
                        )
                    )
                elif len(metrics) == 2:
                    cls.score_file.write(
                        """\n
    try:
        {metric} = float(prediction)
    except TypeError:
        # If the prediction returns as a list of values or improper value type, a TypeError will be raised.
        # Attempt to handle the prediction output in the except block.
        {metric} = float(prediction[0])""".format(
                            metric=metrics[0]
                        )
                    )
                    if threshold is None:
                        threshold = target_df.mean()
                    cls.score_file.write(
                        """\n
    if ({metric0} >= {threshold}):
        {metric1} = '1'
    else:
        {metric1} = '0' """.format(
                            metric0=metrics[0],
                            metric1=metrics[1],
                            threshold=threshold,
                        )
                    )
                elif len(metrics) > 2:
                    for i, metric in enumerate(metrics[:-1]):
                        cls.score_file.write(
                            """\
    {metric} = float(prediction[{i}]""".format(
                                metric=metric, i=i
                            )
                        )
                    cls.score_file.write(
                        """\
    max_prediction = max({metric_list})
    index_prediction = {metric_list}.index(max_prediction)
    {classification} = index_prediction""".format(
                            metric_list=metrics[:-1], classification=metrics[-1]
                        )
                    )
                else:
                    ValueError(
                        "Improper metrics argument was provided. Please provide a list of string metrics."
                    )

            elif is_h2o_model and not is_mlflow:
                cls.score_file.write(
                    """\n
    {} = float(prediction[1][2])
    {} = prediction[1][0]""".format(
                        metrics[0], metrics[1]
                    )
                )
            elif not is_h2o_model and is_mlflow:
                cls.score_file.write(
                    """\n
    {0} = prediction
    if isinstance({0}, np.ndarray):
        {0} = prediction.item(0)""".format(
                        metrics[0]
                    )
                )

            metrics_list = ", ".join(metrics)
            cls.score_file.write(
                """\n
    return({})""".format(
                    metrics_list
                )
            )

            cls.score_file.write("""\n""")

        # For SAS Viya 3.5, the model is first registered to SAS Model Manager, then the model UUID can be
        # added to the score code and reuploaded to the model file contents
        if is_viya35:
            with open(score_code_path, "r") as pFile:
                files = [
                    dict(
                        name="{}Score.py".format(model_prefix), file=pFile, role="score"
                    )
                ]
                upload_and_copy_score_resources(model_id, files)
            # After uploading the score code and migrating score resources, call the wrapper API to create
            # the Python score code wrapped in DS2
            modelRepo.convert_python_to_ds2(model_id)
            # Convert the DS2 wrapper code into two separate files: dmcas_epscorecode.sas and dmcas_packagescorecode.sas
            # The former scores or validates in CAS and the latter in SAS Microanalytic Service
            if score_cas:
                file_contents = modelRepo.get_model_contents(model_id)
                for item in file_contents:
                    if item.name == "score.sas":
                        mas_code = modelRepo.get(
                            "models/%s/contents/%s/content" % (item.modelId, item.id)
                        )
                with open(model_dir / "dmcas_packagescorecode.sas", "w") as file:
                    print(mas_code, file=file)
                cas_code = cls.convert_mas_to_cas(mas_code, model_id)
                with open(model_dir / "dmcas_epscorecode.sas", "w") as file:
                    print(cas_code, file=file)
                for score_code in [
                    "dmcas_packagescorecode.sas",
                    "dmcas_epscorecode.sas",
                ]:
                    with open(model_dir / score_code, "r") as file:
                        if score_code == "dmcas_epscorecode.sas":
                            upload_and_copy_score_resources(
                                model_id, [dict(name=score_code, file=file, role="score")]
                            )
                        else:
                            upload_and_copy_score_resources(
                                model_id, [dict(name=score_code, file=file)]
                            )
                model = modelRepo.get_model(model_id)
                model["scoreCodeType"] = "ds2MultiType"
                modelRepo.update_model(model)

    @classmethod
    def split_string_column(cls, input_series, other_variable):
        """
        Splits a column of string values into a number of new variables equal
        to the number of unique values in the original column (excluding None
        values). It then writes to a file the statements that tokenize the newly
        defined variables.

        Here is an example: Given a series named strCol with values ['A', 'B', 'C',
        None, 'A', 'B', 'A', 'D'], designates the following new variables:
        strCol_A, strCol_B, strCol_D. It then writes the following to the file:
            strCol_A = np.where(val == 'A', 1.0, 0.0)
            strCol_B = np.where(val == 'B', 1.0, 0.0)
            strCol_D = np.where(val == 'D', 1.0, 0.0)

        Parameters
        ---------------
        input_series : string series
            Series with the string dtype.
        cls.score_file : file (class variable)
            Open python file to write into.

        Returns
        ---------------
        new_var_list : string list
            List of all new variable names split from unique values.
        """

        unique_values = input_series.unique()
        unique_values = list(filter(None, unique_values))
        unique_values = [x for x in unique_values if str(x) != "nan"]
        new_var_list = []
        for i, uniq in enumerate(unique_values):
            uniq = uniq.strip()
            if not uniq.isidentifier():
                raise SyntaxError(
                    "Invalid column value in input_data. Values must be "
                    + "valid as Python variables (or easily space strippable)."
                )
            new_var_list.append("{}_{}".format(input_series.name, uniq))
            cls.score_file.write(
                """
    {0} = np.where(categoryStr == '{1}', 1.0, 0.0)""".format(
                    new_var_list[i], uniq
                )
            )

        if ("Other" not in unique_values) and other_variable:
            new_var_list.append("{}_Other".format(input_series.name))
            cls.score_file.write(
                """
    {}_Other = np.where(categoryStr == 'Other', 1.0, 0.0)""".format(
                    input_series.name
                )
            )

        return new_var_list

    @staticmethod
    def check_if_binary(input_series):
        """
        Checks a pandas series to determine whether the values are binary or nominal.

        Parameters
        ---------------
        input_series : float or int series
            A series with numeric values.

        Returns
        ---------------
        is_binary : boolean
            The returned value is True if the series values are binary, and False if the series values
            are nominal.
        """
        is_binary = False
        binary_float = [float(1), float(0)]

        if input_series.value_counts().size == 2:
            if binary_float[0] in input_series.astype("float") and binary_float[
                1
            ] in input_series.astype("float"):
                is_binary = False
            else:
                is_binary = True

        return is_binary

    @staticmethod
    def convert_mas_to_cas(mas_code, model):
        """Using the generated score.sas code from the Python wrapper API,
        convert the SAS Microanalytic Service based code to CAS compatible.

        Parameters
        ----------
        mas_code : str
            String representation of the packagescore.sas DS2 wrapper
        model : str or dict
            The name or id of the model, or a dictionary representation of
            the model

        Returns
        -------
        CASCode : str
            String representation of the epscorecode.sas DS2 wrapper code
        """
        model = modelRepo.get_model(model)
        output_string = ""
        for outVar in model["outputVariables"]:
            output_string = output_string + "dcl "
            if outVar["type"] == "string":
                output_string = output_string + "varchar(100) "
            else:
                output_string = output_string + "double "
            output_string = output_string + outVar["name"] + ";\n"
        start = mas_code.find("score(")
        finish = mas_code[start:].find(");")
        score_vars = mas_code[start + 6: start + finish]
        input_string = " ".join(
            [
                x
                for x in score_vars.split(" ")
                if (x != "double" and x != "in_out" and x != "varchar(100)")
            ]
        )
        end_block = (
            "method run();\n    set SASEP.IN;\n    score({});\nend;\nenddata;".format(
                input_string
            )
        )
        replace_strings = {
            "package pythonScore / overwrite=yes;": "data sasep.out;",
            "dcl int resultCode revision;": "dcl double resultCode revision;\n"
            + output_string,
            "endpackage;": end_block,
        }
        replace_strings = dict((re.escape(k), v) for k, v in replace_strings.items())
        pattern = re.compile("|".join(replace_strings.keys()))
        cas_code = pattern.sub(lambda m: replace_strings[re.escape(m.group(0))], mas_code)
        return cas_code
