#!/usr/bin/env python
# encoding: utf-8
#
# Copyright Â© 2023, SAS Institute Inc., Cary, NC, USA.  All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import json
import pytest
import random
import tempfile
from pathlib import Path

import numpy as np
import pandas as pd

import sasctl.pzmm as pzmm
from sasctl.pzmm.write_json_files import JSONFiles as jf

# Example input variable list from hmeq dataset (generated by mlflow_model.py)
input_dict = [
    {"name": "LOAN", "type": "long"},
    {"name": "MORTDUE", "type": "double"},
    {"name": "VALUE", "type": "double"},
    {"name": "YOJ", "type": "double"},
    {"name": "DEROG", "type": "double"},
    {"name": "DELINQ", "type": "double"},
    {"name": "CLAGE", "type": "double"},
    {"name": "NINQ", "type": "double"},
    {"name": "CLNO", "type": "double"},
    {"name": "DEBTINC", "type": "double"},
    {"name": "JOB_Office", "type": "integer"},
    {"name": "JOB_Other", "type": "integer"},
    {"name": "JOB_ProfExe", "type": "integer"},
    {"name": "JOB_Sales", "type": "integer"},
    {"name": "JOB_Self", "type": "integer"},
    {"name": "REASON_HomeImp", "type": "integer"},
]


def test_flatten_list():
    """
    Test cases:
    - Single list of strings
    - Nested list of strings
    - Nested list of ints
    """
    str_list = ["a", "bad", "good", "Z"]
    assert list(pzmm.write_json_files._flatten(str_list)) == str_list

    nested_str = ["a", ["bad", "good", "Z"]]
    assert list(pzmm.write_json_files._flatten(nested_str)) == str_list

    nested_int = [1, 2, [3, 4, 5], 6, 7, [8, 9]]
    int_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    assert list(pzmm.write_json_files._flatten(nested_int)) == int_list


def test_check_if_string():
    """
    Test cases:
    - If/else statements
    """
    data = [
        {"type": "string"},
        {"type": "double"},
        {"type": "tensor", "tensor-spec": {"dtype": "string"}},
        {"type": "tensor", "tensor-spec": {"dtype": "double"}},
    ]
    test = [True, False, True, False]
    for d, t in zip(data, test):
        assert jf.check_if_string(d) is t


def test_generate_variable_properties(hmeq_dataset):
    """
    Test cases:
    - Return expected variable properties from hmeq dataset
    - Include case of passing a series
    """
    df = hmeq_dataset
    # Generate the variable properties from the dataset (excluding the target variable)
    dict_list = jf.generate_variable_properties(df.drop(["BAD"], axis=1))
    # Find all expected variables
    assert len(dict_list) == 12
    # Verify expected variable properties
    for var in dict_list:
        if var["name"] in ["REASON", "JOB"]:
            assert var["level"] == "nominal" and var["type"] == "string"
        else:
            assert var["level"] == "interval" and var["type"] == "decimal"

    df = pd.Series([1, 2, 3, 4, 5], dtype="category", name="cat")
    dict_list = jf.generate_variable_properties(df)
    assert len(dict_list) == 1
    assert dict_list[0]["level"] == "nominal" and dict_list[0]["type"] == "decimal"


def test_generate_mlflow_variable_properties():
    """
    Test cases:
    - Return expected number of variables from mlflow_model.py output
    """
    dict_list = jf.generate_mlflow_variable_properties(input_dict)
    assert len(dict_list) == 16


def test_write_var_json(hmeq_dataset):
    """
    Test cases:
    - Generate correctly named file based on is_input (assuming path provided)
    - Return correctly labelled dict based on is_input (assuming no path provided)
    - Return correctly labelled dict from Mlflow model dataset
    """
    df = hmeq_dataset
    with tempfile.TemporaryDirectory() as tmp_dir:
        jf.write_var_json(df, True, Path(tmp_dir))
        assert (Path(tmp_dir) / "inputVar.json").exists()
        jf.write_var_json(df, False, Path(tmp_dir))
        assert (Path(tmp_dir) / "outputVar.json").exists()

    var_dict = jf.write_var_json(df, False)
    assert "outputVar.json" in var_dict

    var_mlflow_dict = jf.write_var_json(input_dict, True)
    assert "inputVar.json" in var_mlflow_dict


def test_write_model_properties_json():
    """
    Test Cases:
    - Generate correctly named file with json_path provided
    - Return correctly labelled dict when no json_path is provided
    - Truncate model description that is too long
    - Interval target_level and invalid event_prob_var
    """
    with tempfile.TemporaryDirectory() as tmp_dir:
        jf.write_model_properties_json(
            model_name="Test_Model",
            target_variable="BAD",
            target_event="1",
            num_target_categories=2,
            json_path=Path(tmp_dir),
        )
        assert (Path(tmp_dir) / "ModelProperties.json").exists()

    prop_dict = jf.write_model_properties_json(
        model_name="Test_Model",
        target_variable="BAD",
        target_event="1",
        num_target_categories=2,
    )
    assert "ModelProperties.json" in prop_dict

    prop_dict = jf.write_model_properties_json(
        model_name="Test_Model",
        target_variable="BAD",
        target_event="1",
        num_target_categories=3,
        model_desc="a" * 2000,
    )
    assert len(json.loads(prop_dict["ModelProperties.json"])["description"]) <= 1024
    assert json.loads(prop_dict["ModelProperties.json"])["target_level"] == "INTERVAL"


def test_write_file_metadata_json():
    """
    Test cases:
    - Generate correctly named file with json_path provided
    - Return correctly labelled dict when no json_path is provided
    - Proper score resource name for H2O.ai model
    """
    with tempfile.TemporaryDirectory() as tmp_dir:
        jf.write_file_metadata_json(model_prefix="Test_Model", json_path=Path(tmp_dir))
        assert (Path(tmp_dir) / "fileMetadata.json").exists()

    meta_dict = jf.write_file_metadata_json(model_prefix="Test_Model")
    assert "fileMetadata.json" in meta_dict

    meta_dict = jf.write_file_metadata_json(
        model_prefix="Test_Model", is_h2o_model=True
    )
    assert json.loads(meta_dict["fileMetadata.json"])[3]["name"] == "Test_Model.mojo"


def test_add_tuple_to_fitstat():
    """
    Test cases:
    - Raise error if non-tuple provided in list
    - Raise error if tuple provided has an invalid shape
    - Updated data_map is returned with a valid tuple list
    - Produce a warning if an invalid parameter is provided, but still complete function
    """
    invalid_tuple = 1
    tuple_list = [invalid_tuple]
    with pytest.raises(
        ValueError,
        match=f"Expected a tuple, but got {str(type(invalid_tuple))} instead.",
    ):
        jf.add_tuple_to_fitstat([], tuple_list, [])

    invalid_tuple = (1, 2, 3, 4, 5)
    tuple_list = [invalid_tuple]
    with pytest.raises(
        ValueError,
        match=f"Expected a tuple with three parameters, but instead got tuple with "
        f"length {len(invalid_tuple)} ",
    ):
        jf.add_tuple_to_fitstat([], tuple_list, [])

    valid_params = ["_RASE_", "_NObs_"]
    data_map = [
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
    ]
    tuple_list = [("RASE", 10, 1), ("_NObs_", 33, "TEST")]
    new_map = jf.add_tuple_to_fitstat(data_map, tuple_list, valid_params)
    assert new_map[0]["dataMap"]["_RASE_"] == 10
    assert new_map[1]["dataMap"]["_NObs_"] == 33

    invalid_param = "BAD"
    tuple_list.append((invalid_param, 0, 2))
    with pytest.warns(
        UserWarning,
        match=f"WARNING: {invalid_param} is not a valid parameter and has been ignored.",
    ):
        warn_map = jf.add_tuple_to_fitstat(data_map, tuple_list, valid_params)
        assert warn_map[0]["dataMap"]["_RASE_"] == 10
        assert warn_map[1]["dataMap"]["_NObs_"] == 33


def test_user_input_fitstat(monkeypatch):
    """
    Test cases:
    - Produce a warning for invalid parameters, then stop input
    - Produce a warning for invalid role, then stop input
    - Updated data_map is returned with inputted values
    """

    def mock_input(prompt):
        return next(input_value)

    monkeypatch.setattr("builtins.input", mock_input)
    data_map = [
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
    ]
    valid_params = ["_RASE_", "_NObs_"]
    invalid_param = "BAD"
    with pytest.warns(UserWarning, match=f"{invalid_param} is not a valid parameter."):
        input_value = iter([invalid_param, "Y", invalid_param, "N"])
        warn1_map = jf.user_input_fitstat(data_map, valid_params)
        assert warn1_map == data_map

    invalid_role = 5
    with pytest.warns(
        UserWarning,
        match=f"{invalid_role} is not a valid role value. It should be either 1, 2, or "
        f"3 or TRAIN, TEST, or VALIDATE respectively.",
    ):
        input_value = iter(
            ["_NObs_", 10, invalid_role, "Y", "_NObs_", 10, invalid_role, "N"]
        )
        warn2_map = jf.user_input_fitstat(data_map, valid_params)
        assert warn2_map == data_map

    input_value = iter(["_RASE_", 10, 1, "Y", "_NObs_", 33, "TEST", "N"])
    new_map = jf.user_input_fitstat(data_map, valid_params)
    assert new_map[0]["dataMap"]["_RASE_"] == 10
    assert new_map[1]["dataMap"]["_NObs_"] == 33


def test_add_df_to_fitstat():
    """
    Test cases:
    - Produce warning for invalid parameter, but still return a data map
    - Produce warning for invalid role, but still return a data map
    - Updated data map is returned with valid values
    """
    data_map = [
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
        {"dataMap": {"_RASE_": None, "_NObs_": None}},
    ]
    invalid_param = "BAD"
    valid_params = ["_RASE_", "_NObs_"]
    df = pd.DataFrame(data=[[invalid_param, 10, 1]])
    with pytest.warns(UserWarning, match=f"{invalid_param} is not a valid parameter."):
        warn1_map = jf.add_df_to_fitstat(df, data_map, valid_params)
        assert data_map == warn1_map

    invalid_role = 5
    with pytest.warns(
        UserWarning,
        match=f"{invalid_role} is not a valid role value. It should be either 1, "
        f"2, or 3 or TRAIN, TEST, or VALIDATE respectively.",
    ):
        df = pd.DataFrame(data=[["_NObs_", 10, invalid_role]])
        warn2_map = jf.add_df_to_fitstat(df, data_map, valid_params)
        assert warn2_map == data_map

    df = pd.DataFrame(data=[["_RASE_", 10, 1], ["_NObs_", 33, "TEST"]])
    new_map = jf.add_df_to_fitstat(df, data_map, valid_params)
    assert new_map[0]["dataMap"]["_RASE_"] == 10
    assert new_map[1]["dataMap"]["_NObs_"] == 33


def test_input_fit_statistics(monkeypatch):
    """
    Test cases:
    - Generate correctly named file with json_path provided (tuple list)
    - Return correctly labelled dict when no json_path is provided (tuple list)
    - Repeat above with user input
    - Repeat above with input Dataframe
    """
    tuple_list = [("RASE", 10, 1), ("_NObs_", 33, "TEST")]
    with tempfile.TemporaryDirectory() as tmp_dir:
        jf.input_fit_statistics(tuple_list=tuple_list, json_path=Path(tmp_dir))
        assert (Path(tmp_dir) / "dmcas_fitstat.json").exists()

    fitstat_dict = jf.input_fit_statistics(tuple_list=tuple_list)
    assert "dmcas_fitstat.json" in fitstat_dict

    fitstat_dict = jf.input_fit_statistics(fitstat_df=pd.DataFrame(data=tuple_list))
    assert "dmcas_fitstat.json" in fitstat_dict

    def mock_input(prompt):
        return next(input_value)

    monkeypatch.setattr("builtins.input", mock_input)
    input_value = iter(["_RASE_", 10, 1, "Y", "_NObs_", 33, "TEST", "N"])
    fitstat_dict = jf.input_fit_statistics(user_input=True)
    assert "dmcas_fitstat.json" in fitstat_dict


def test_check_for_data():
    """
    Test cases:
    - Raise error when no data provided
    - Check bool returns for different data arguments
    """
    with pytest.raises(
        ValueError,
        match="No data was provided. Please provide the actual and predicted values "
        r"for at least one of the partitions \(VALIDATE, TRAIN, or TEST\).",
    ):
        jf.check_for_data(None, None, None)

    assert jf.check_for_data(1, 2, 3) == [1, 1, 1]
    assert jf.check_for_data(1, None, None) == [1, 0, 0]


def test_stat_dataset_to_dataframe():
    """
    Test cases:
    - Raise ValueError if improper data is provided
    - Convert data from each type to proper dataframe
    - Create normalized probabilities for 2 column inputs (verify p E [0,1])
    """
    with pytest.raises(
        ValueError,
        match="Please provide the data in a list of lists, dataframe, or numpy "
        "array.",
    ):
        jf.stat_dataset_to_dataframe((1, 2), 1)

    dummy_target_value = 5
    dummy_actual = [float(random.randint(0, 10)) for _ in range(10)]
    dummy_predict = [float(random.randint(0, 10)) for _ in range(10)]
    dummy_proba = [x / max(dummy_predict) for x in dummy_predict]

    expected = pd.DataFrame(
        {"actual": dummy_actual, "predict": dummy_predict, "predict_proba": dummy_proba}
    )
    expected_binary = expected.drop(["predict_proba"], axis=1)
    expected_binary["predict_proba"] = (
        expected["predict"].gt(dummy_target_value).astype(int)
    )
    assert expected_binary["predict_proba"].between(0, 1).any()

    # pandas DataFrame
    df = pd.DataFrame({"a": dummy_actual, "p": dummy_predict, "pr": dummy_proba})
    assert jf.stat_dataset_to_dataframe(df).equals(expected)
    assert jf.stat_dataset_to_dataframe(
        df.drop(["predict_proba"], axis=1), dummy_target_value
    ).equals(expected_binary)

    # list of lists
    ll = [dummy_actual, dummy_predict, dummy_proba]
    assert jf.stat_dataset_to_dataframe(ll).equals(expected)
    assert jf.stat_dataset_to_dataframe(ll[0:2], dummy_target_value).equals(
        expected_binary
    )

    # numpy array
    array = np.array(ll)
    assert jf.stat_dataset_to_dataframe(array).equals(expected)
    assert jf.stat_dataset_to_dataframe(array[0:2], dummy_target_value).equals(
        expected_binary
    )
